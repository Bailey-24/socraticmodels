{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHsncwrPOxZt"
   },
   "source": [
    "Copyright 2021 Google LLC.\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "# **Socratic Models: Image Captioning**\n",
    "\n",
    "Socratic Models (SMs) is a framework that composes multiple pre-existing foundation models (e.g., large language models, visual language models, audio-language models) to provide results for new multimodal tasks, without any model finetuning.\n",
    "\n",
    "This colab runs an example of SMs for image captioning.\n",
    "\n",
    "This is a reference implementation of one task demonstrated in the work: [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://socraticmodels.github.io/)\n",
    "\n",
    "**Disclaimer:** this colab uses CLIP and GPT-3 as foundation models, and may be subject to unwanted biases. This code should be used with caution (and checked for correctness) in downstream applications.\n",
    "\n",
    "### **Quick Start:**\n",
    "\n",
    "**Step 1.** Register for an [OpenAI API key](https://openai.com/blog/openai-api/) to use GPT-3 (there's a free trial) and enter it below\n",
    "\n",
    "**Step 2.** Menu > Change runtime type > Hardware accelerator > \"GPU\"\n",
    "\n",
    "**Step 3.** Menu > Runtime > Run all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5zu8FrCf46t"
   },
   "outputs": [],
   "source": [
    "openai_api_key = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q39CIF2h3RYB"
   },
   "source": [
    "## **Setup**\n",
    "This installs a few dependencies: PyTorch, CLIP, GPT-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9Mu4swCz6iS",
    "outputId": "574dfd78-b382-4a94-9069-88414000fb66"
   },
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm fvcore imageio imageio-ffmpeg openai pattern\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install -U --no-cache-dir gdown --pre\n",
    "!pip install profanity-filter\n",
    "!nvidia-smi  # Show GPU info.\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpR4dgevMMsa"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import clip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "from PIL import Image\n",
    "from profanity_filter import ProfanityFilter\n",
    "import torch\n",
    "\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RjOSbFKzGzq"
   },
   "source": [
    "## **Foundation Models**\n",
    "Select which foundation models to use.\n",
    "\n",
    "**Defaults:** [CLIP](https://arxiv.org/abs/2103.00020) VIT-L/14 as the VLM, and [GPT-3](https://arxiv.org/abs/2005.14165) \"Davinci\" as the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "37NFqO2hzUBI"
   },
   "outputs": [],
   "source": [
    "clip_version = \"ViT-L/14\" #@param [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\", \"RN50x64\", \"ViT-B/32\", \"ViT-B/16\", \"ViT-L/14\"] {type:\"string\"}\n",
    "gpt_version = \"text-davinci-002\" #@param [\"text-davinci-001\", \"text-davinci-002\", \"text-curie-001\", \"text-babbage-001\", \"text-ada-001\"] {type:\"string\"}\n",
    "\n",
    "clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768, 'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ln3p3jE0taA"
   },
   "source": [
    "## **Getting Started**\n",
    "Download CLIP model weights, and define helper functions. This might take a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UsyY9_zRGDn"
   },
   "source": [
    "##### Download [CLIP](https://arxiv.org/abs/2103.00020) model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Wk8r4Jd4xkY",
    "outputId": "7416bcfb-360c-4516-c721-614d59c02fd5"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.set_per_process_memory_fraction(0.9, None)  # Only needed if session crashes.\n",
    "model, preprocess = clip.load(clip_version)  # clip.available_models()\n",
    "model.cuda().eval()\n",
    "\n",
    "def num_params(model):\n",
    "  return np.sum([int(np.prod(p.shape)) for p in model.parameters()])\n",
    "print(\"Model parameters (total):\", num_params(model))\n",
    "print(\"Model parameters (image encoder):\", num_params(model.visual))\n",
    "print(\"Model parameters (text encoder):\", num_params(model.token_embedding) + num_params(model.transformer))\n",
    "print(\"Input image resolution:\", model.visual.input_resolution)\n",
    "print(\"Context length:\", model.context_length)\n",
    "print(\"Vocab size:\", model.vocab_size)\n",
    "img_size = model.visual.input_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blE3UYVYearl"
   },
   "source": [
    "##### Define CLIP helper functions (e.g., nearest neighbor search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIEXUpuRjDKw"
   },
   "outputs": [],
   "source": [
    "def get_text_feats(in_text, batch_size=64):\n",
    "  text_tokens = clip.tokenize(in_text).cuda()\n",
    "  text_id = 0\n",
    "  text_feats = np.zeros((len(in_text), clip_feat_dim), dtype=np.float32)\n",
    "  while text_id < len(text_tokens):  # Batched inference.\n",
    "    batch_size = min(len(in_text) - text_id, batch_size)\n",
    "    text_batch = text_tokens[text_id:text_id+batch_size]\n",
    "    with torch.no_grad():\n",
    "      batch_feats = model.encode_text(text_batch).float()\n",
    "    batch_feats /= batch_feats.norm(dim=-1, keepdim=True)\n",
    "    batch_feats = np.float32(batch_feats.cpu())\n",
    "    text_feats[text_id:text_id+batch_size, :] = batch_feats\n",
    "    text_id += batch_size\n",
    "  return text_feats\n",
    "\n",
    "def get_img_feats(img):\n",
    "  img_pil = Image.fromarray(np.uint8(img))\n",
    "  img_in = preprocess(img_pil)[None, ...]\n",
    "  with torch.no_grad():\n",
    "    img_feats = model.encode_image(img_in.cuda()).float()\n",
    "  img_feats /= img_feats.norm(dim=-1, keepdim=True)\n",
    "  img_feats = np.float32(img_feats.cpu())\n",
    "  return img_feats\n",
    "\n",
    "def get_nn_text(raw_texts, text_feats, img_feats):\n",
    "  scores = text_feats @ img_feats.T\n",
    "  scores = scores.squeeze()\n",
    "  high_to_low_ids = np.argsort(scores).squeeze()[::-1]\n",
    "  high_to_low_texts = [raw_texts[i] for i in high_to_low_ids]\n",
    "  high_to_low_scores = np.sort(scores).squeeze()[::-1]\n",
    "  return high_to_low_texts, high_to_low_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUL-mu57Q33w"
   },
   "source": [
    "##### Define [GPT-3](https://arxiv.org/abs/2005.14165) helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGcJT4aJQzi6"
   },
   "outputs": [],
   "source": [
    "def prompt_llm(prompt, max_tokens=64, temperature=0, stop=None):\n",
    "  response = openai.Completion.create(engine=gpt_version, prompt=prompt, max_tokens=max_tokens, temperature=temperature, stop=stop)\n",
    "  return response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC4AuVQMg4KE"
   },
   "source": [
    "##### Load scene categories from [Places365](http://places2.csail.mit.edu/download.html) and compute their CLIP features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qczjVeUrg0NI",
    "outputId": "b1e6a25b-7185-4453-bf40-9b53c72ed7b4"
   },
   "outputs": [],
   "source": [
    "# Load scene categories from Places365.\n",
    "if not os.path.exists('categories_places365.txt'):\n",
    "  ! wget https://raw.githubusercontent.com/zhoubolei/places_devkit/master/categories_places365.txt\n",
    "place_categories = np.loadtxt('categories_places365.txt', dtype=str)\n",
    "place_texts = []\n",
    "for place in place_categories[:, 0]:\n",
    "  place = place.split('/')[2:]\n",
    "  if len(place) > 1:\n",
    "    place = place[1] + ' ' + place[0]\n",
    "  else:\n",
    "    place = place[0]\n",
    "  place = place.replace('_', ' ')\n",
    "  place_texts.append(place)\n",
    "place_feats = get_text_feats([f'Photo of a {p}.' for p in place_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMslxlrCSFnH"
   },
   "source": [
    "##### Load object categories from [Tencent ML Images](https://arxiv.org/pdf/1901.01703.pdf) and compute their CLIP features. This might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hY7uogw3Q3_4",
    "outputId": "ec91b4a7-90fa-4736-e772-d2380f188db3"
   },
   "outputs": [],
   "source": [
    "# Load object categories from Tencent ML Images.\n",
    "if not os.path.exists('dictionary_and_semantic_hierarchy.txt'):\n",
    "  ! wget https://raw.githubusercontent.com/Tencent/tencent-ml-images/master/data/dictionary_and_semantic_hierarchy.txt\n",
    "with open('dictionary_and_semantic_hierarchy.txt') as fid:\n",
    "    object_categories = fid.readlines()\n",
    "object_texts = []\n",
    "pf = ProfanityFilter()\n",
    "for object_text in object_categories[1:]:\n",
    "  object_text = object_text.strip()\n",
    "  object_text = object_text.split('\\t')[3]\n",
    "  safe_list = ''\n",
    "  for variant in object_text.split(','):\n",
    "    text = variant.strip()\n",
    "    if pf.is_clean(text):\n",
    "      safe_list += f'{text}, '\n",
    "  safe_list = safe_list[:-2]\n",
    "  if len(safe_list) > 0:\n",
    "    object_texts.append(safe_list)\n",
    "object_texts = [o for o in list(set(object_texts)) if o not in place_texts]  # Remove redundant categories.\n",
    "object_feats = get_text_feats([f'Photo of a {o}.' for o in object_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLq1cyarUFs3"
   },
   "source": [
    "## **Demo:** Image Captioning\n",
    "Run image captioning on an Internet image (linked via URL).\n",
    "\n",
    "**Note:** due to the non-zero temperature used for sampling from the generative language model, results from this approach are stochastic, but comparable results are producible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "id": "cHH4ApETu5gD",
    "outputId": "56897920-7567-4ba5-e164-af8dbd17581b"
   },
   "outputs": [],
   "source": [
    "# Download image.\n",
    "img_url = \"https://github.com/rmokady/CLIP_prefix_caption/raw/main/Images/COCO_val2014_000000165547.jpg\" #@param {type:\"string\"}\n",
    "fname = 'demo_img.png'\n",
    "with open(fname, 'wb') as f:\n",
    "    f.write(requests.get(img_url).content)\n",
    "\n",
    "verbose = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Load image.\n",
    "img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n",
    "img_feats = get_img_feats(img)\n",
    "plt.imshow(img); plt.show()\n",
    "\n",
    "# Zero-shot VLM: classify image type.\n",
    "img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "img_types_feats = get_text_feats([f'This is a {t}.' for t in img_types])\n",
    "sorted_img_types, img_type_scores = get_nn_text(img_types, img_types_feats, img_feats)\n",
    "img_type = sorted_img_types[0]\n",
    "\n",
    "# Zero-shot VLM: classify number of people.\n",
    "ppl_texts = ['no people', 'people']\n",
    "ppl_feats = get_text_feats([f'There are {p} in this photo.' for p in ppl_texts])\n",
    "sorted_ppl_texts, ppl_scores = get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "ppl_result = sorted_ppl_texts[0]\n",
    "if ppl_result == 'people':\n",
    "  ppl_texts = ['is one person', 'are two people', 'are three people', 'are several people', 'are many people']\n",
    "  ppl_feats = get_text_feats([f'There {p} in this photo.' for p in ppl_texts])\n",
    "  sorted_ppl_texts, ppl_scores = get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "  ppl_result = sorted_ppl_texts[0]\n",
    "else:\n",
    "  ppl_result = f'are {ppl_result}'\n",
    "\n",
    "# Zero-shot VLM: classify places.\n",
    "place_topk = 3\n",
    "place_feats = get_text_feats([f'Photo of a {p}.' for p in place_texts ])\n",
    "sorted_places, places_scores = get_nn_text(place_texts, place_feats, img_feats)\n",
    "\n",
    "# Zero-shot VLM: classify objects.\n",
    "obj_topk = 10\n",
    "sorted_obj_texts, obj_scores = get_nn_text(object_texts, object_feats, img_feats)\n",
    "object_list = ''\n",
    "for i in range(obj_topk):\n",
    "  object_list += f'{sorted_obj_texts[i]}, '\n",
    "object_list = object_list[:-2]\n",
    "\n",
    "# Zero-shot LM: generate captions.\n",
    "num_captions = 10\n",
    "prompt = f'''I am an intelligent image captioning bot.\n",
    "This image is a {img_type}. There {ppl_result}.\n",
    "I think this photo was taken at a {sorted_places[0]}, {sorted_places[1]}, or {sorted_places[2]}.\n",
    "I think there might be a {object_list} in this {img_type}.\n",
    "A creative short caption I can generate to describe this image is:'''\n",
    "caption_texts = [prompt_llm(prompt, temperature=0.9) for _ in range(num_captions)]\n",
    "\n",
    "# Zero-shot VLM: rank captions.\n",
    "caption_feats = get_text_feats(caption_texts)\n",
    "sorted_captions, caption_scores = get_nn_text(caption_texts, caption_feats, img_feats)\n",
    "print(f'{sorted_captions[0]}\\n')\n",
    "\n",
    "if verbose:\n",
    "  print(f'VLM: This image is a:')\n",
    "  for img_type, score in zip(sorted_img_types, img_type_scores):\n",
    "    print(f'{score:.4f} {img_type}')\n",
    "\n",
    "  print(f'\\nVLM: There:')\n",
    "  for ppl_text, score in zip(sorted_ppl_texts, ppl_scores):\n",
    "    print(f'{score:.4f} {ppl_text}')\n",
    "\n",
    "  print(f'\\nVLM: I think this photo was taken at a:')\n",
    "  for place, score in zip(sorted_places[:place_topk], places_scores[:place_topk]):\n",
    "    print(f'{score:.4f} {place}')\n",
    "\n",
    "  print(f'\\nVLM: I think there might be a:')\n",
    "  for obj_text, score in zip(sorted_obj_texts[:obj_topk], obj_scores[:obj_topk]):\n",
    "    print(f'{score:.4f} {obj_text}')\n",
    "\n",
    "  print(f'\\nLM generated captions ranked by VLM scores:')\n",
    "  for caption, score in zip(sorted_captions, caption_scores):\n",
    "    print(f'{score:.4f} {caption}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Q39CIF2h3RYB",
    "0UsyY9_zRGDn",
    "blE3UYVYearl",
    "bUL-mu57Q33w",
    "AC4AuVQMg4KE",
    "TMslxlrCSFnH"
   ],
   "machine_shape": "hm",
   "name": "SocraticModels-ImageCaptioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
