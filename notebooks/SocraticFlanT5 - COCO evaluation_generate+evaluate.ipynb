{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655ff268",
   "metadata": {},
   "source": [
    "# SocraticFlanT5 - Evaluation on MS COCO | DL2 Mini-project, May 2023\n",
    "---\n",
    "\n",
    "This notebook downloads the images from the validation split of the [MS COCO Dataset (2017 version)](https://cocodataset.org/#download) and the corresponding ground-truth captions, generates captions based on the Socratic model pipeline outlined below, and evaluates the generated captions based on the MS COCO ground-truth captions. We will evaluate the folowing two approaches: \n",
    "1. Baseline: a Socratic model based on the work by [Zeng et al. (2022)](https://socraticmodels.github.io/) where GPT-3 is replaced by [FLAN-T5-xl](https://huggingface.co/docs/transformers/model_doc/flan-t5). \n",
    "\n",
    "2. Improved prompting: an improved baseline model where the template prompt filled by CLIP is processed before passing to FLAN-T5-xl.\n",
    "\n",
    "There are two approaches to this evaluation: rule-based and embedding-based.\n",
    "\n",
    "---\n",
    "For the **rule-based approach**, the following metrics will be used, based on [this](https://github.com/salaniz/pycocoevalcap) repository:\n",
    "\n",
    "* *BLEU-4*: BLEU (Bilingual Evaluation Understudy) is a metric that measures the similarity between the generated captions and the ground truth captions based on n-gram matching. The BLEU-4 score measures the precision of the generated captions up to four-grams compared to the ground truth captions.\n",
    "\n",
    "* *METEOR*: METEOR (Metric for Evaluation of Translation with Explicit ORdering) is another metric that measures the similarity between the generated captions and the ground truth captions. It also takes into account word order and synonymy by using a set of reference summaries to compute a harmonic mean of precision and recall.\n",
    "\n",
    "* *CIDEr*: CIDEr (Consensus-based Image Description Evaluation) is a metric that measures the consensus between the generated captions and the ground truth captions. It computes the similarity between the generated captions and the reference captions based on their TF-IDF weights, which helps capture important words in the captions.\n",
    "\n",
    "* *SPICE*: SPICE (Semantic Propositional Image Caption Evaluation) is a metric that measures the semantic similarity between the generated captions and the ground truth captions. It analyzes the similarity between the semantic propositions present in the generated captions and those in the reference captions, taking into account the structure and meaning of the propositions.\n",
    "\n",
    "* *ROUGE-L*: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric that measures the similarity between the generated captions and the ground truth captions based on overlapping sequences of words. ROUGE-L measures the longest common subsequence (LCS) between the generated captions and the reference captions, taking into account sentence-level structure and word order.\n",
    "\n",
    "---\n",
    "\n",
    "For the **embedding-based** approach (based on CLIP embeddings), we calculate the cosine similarities between each image embedding and embeddings of the ground truth captions and then we calculate the cosine similarities between each image embedding and embeddings of the captions generated with FLAN-T5-xl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e331e",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1503d1d",
   "metadata": {},
   "source": [
    "#### Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11989003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.image_captioning import ClipManager, ImageManager, VocabManager, FlanT5Manager, COCOManager\n",
    "from scripts.eval import SocraticEvalCap\n",
    "from scripts.utils import get_device\n",
    "from transformers import set_seed\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d341a53",
   "metadata": {},
   "source": [
    "#### Set seeds for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HuggingFace seed\n",
    "set_seed(42)\n",
    "\n",
    "# Set seed for 100 random images of the MS COCO validation split\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada75e",
   "metadata": {},
   "source": [
    "### Step 1: Downloading the MS COCO images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898c7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_manager = COCOManager()\n",
    "coco_manager.download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c54da",
   "metadata": {},
   "source": [
    "### Step 2: Generating the captions via the Socratic pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aab5ad",
   "metadata": {},
   "source": [
    "#### Set the device and instantiate managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bcac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 0.0s!\n"
     ]
    }
   ],
   "source": [
    "# Set the device to use\n",
    "device = get_device()\n",
    "\n",
    "# Instantiate the clip manager\n",
    "clip_manager = ClipManager(device)\n",
    "\n",
    "# Instantiate the image manager\n",
    "image_manager = ImageManager()\n",
    "\n",
    "# Instantiate the vocab manager\n",
    "vocab_manager = VocabManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0d924",
   "metadata": {},
   "source": [
    "#### Compute place and object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372c21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the place features\n",
    "if not os.path.exists('cache/place_feats.npy'):\n",
    "\n",
    "    # Calculate the place features\n",
    "    place_feats = clip_manager.get_text_feats([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "    np.save('cache/place_feats.npy', place_feats)\n",
    "else:\n",
    "    place_feats = np.load('cache/place_feats.npy')\n",
    "\n",
    "# Calculate the object features\n",
    "if not os.path.exists('cache/object_feats.npy'):\n",
    "    # Calculate the object features\n",
    "    object_feats = clip_manager.get_text_feats([f'Photo of a {o}.' for o in vocab_manager.object_list])\n",
    "    np.save('cache/object_feats.npy', object_feats)\n",
    "else:\n",
    "    object_feats = np.load('cache/object_feats.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb0b09",
   "metadata": {},
   "source": [
    "#### Define the parameters of the template prompt passed to the VLM (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31649a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What approach would you like to use to generate captions? [baseline/improved_model]? baseline\n"
     ]
    }
   ],
   "source": [
    "approach = input('What approach would you like to use to generate captions? [baseline/improved_model]? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1bc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_text_feats starting!\n",
      "get_text_feats took 0.8s!\n",
      "get_text_feats starting!\n",
      "get_text_feats took 0.1s!\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot VLM: classify image type\n",
    "img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "img_types_feats = clip_manager.get_text_feats([f'This is a {t}.' for t in img_types])\n",
    "\n",
    "# Zero-shot VLM: classify number of people\n",
    "if approach == 'baseline':\n",
    "    ppl_texts = ['no people', 'people']\n",
    "else:\n",
    "    ppl_texts = [\n",
    "    'are no people', 'is one person', 'are two people', 'are three people', 'are several people', 'are many people'\n",
    "]\n",
    "ppl_feats = clip_manager.get_text_feats([f'There are {p} in this photo.' for p in ppl_texts])\n",
    "\n",
    "# Zero-shot VLM: how many top places are returned by the VLM\n",
    "place_topk = 3\n",
    "\n",
    "# Zero-shot VLM: how many top objects are returned by the VLM\n",
    "obj_topk = 10\n",
    "\n",
    "# Zero-shot LM: how many captions are generated\n",
    "num_captions = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e84fa",
   "metadata": {},
   "source": [
    "#### Generate image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6eec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to caption a single image\n",
    "def caption_this_image(filename, imgs_folder, ix, random_numbers, image_manager, clip_manager, vocab_manager, flan_manager):\n",
    "    start_time = time.time()\n",
    "    # Consider only image files that are part of the random sample\n",
    "    if file_name.endswith(\".jpg\") and ix in random_numbers:  \n",
    "        # Getting image id\n",
    "        file_name_strip = file_name.strip('.jpg')\n",
    "        match = re.search('^0+', file_name_strip)\n",
    "        sequence = match.group(0)\n",
    "        image_id = int(file_name_strip[len(sequence):])\n",
    "\n",
    "        img_path = os.path.join(imgs_folder, file_name)\n",
    "        img = image_manager.load_image(img_path)\n",
    "        img_feats = clip_manager.get_img_feats(img)\n",
    "        img_feats = img_feats.flatten()\n",
    "        \n",
    "\n",
    "        # Zero-shot VLM: classify image type.\n",
    "        sorted_img_types, img_type_scores = clip_manager.get_nn_text(img_types, img_types_feats, img_feats)\n",
    "        img_type = sorted_img_types[0]\n",
    "\n",
    "        # Zero-shot VLM: classify number of people.\n",
    "        sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "        ppl_result = sorted_ppl_texts[0]\n",
    "        if approach == 'baseline':\n",
    "            if ppl_result == 'people':\n",
    "                ppl_texts = ['is one person', 'are two people', 'are three people', 'are several people', 'are many people']\n",
    "                ppl_feats = clip_manager.get_text_feats([f'There {p} in this photo.' for p in ppl_texts])\n",
    "                sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "                ppl_result = sorted_ppl_texts[0]\n",
    "            else:\n",
    "                ppl_result = f'are {ppl_result}'\n",
    "\n",
    "        # Zero-shot VLM: classify places.\n",
    "        sorted_places, places_scores = clip_manager.get_nn_text(vocab_manager.place_list, place_feats, img_feats)\n",
    "        if approach != 'baseline':\n",
    "            place_score_map = dict(zip(sorted_places, places_scores))\n",
    "\n",
    "        \n",
    "        # Zero-shot VLM: classify objects.\n",
    "        sorted_obj_texts, obj_scores = clip_manager.get_nn_text(vocab_manager.object_list, object_feats, img_feats)\n",
    "        object_list = ''\n",
    "        for i in range(obj_topk):\n",
    "            object_list += f'{sorted_obj_texts[i]}, '\n",
    "            object_list = object_list[:-2]\n",
    "        if approach != 'baseline':\n",
    "            object_score_map = dict(zip(sorted_obj_texts, obj_scores))\n",
    "\n",
    "        if approach == 'baseline':\n",
    "            # Zero-shot LM: generate captions.\n",
    "            prompt = f'''I am an intelligent image captioning bot.\n",
    "            This image is a {img_type}. There {ppl_result}.\n",
    "            I think this photo was taken at a {sorted_places[0]}, {sorted_places[1]}, or {sorted_places[2]}.\n",
    "            I think there might be a {object_list} in this {img_type}.\n",
    "            A creative short caption I can generate to describe this image is:'''\n",
    "\n",
    "            # Generate multiple captions\n",
    "            model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "            caption_texts = flan_manager.generate_response(num_captions * [prompt], model_params)\n",
    "\n",
    "            # Zero-shot VLM: rank captions.\n",
    "            caption_feats = clip_manager.get_text_feats(caption_texts)\n",
    "            sorted_captions, caption_scores = clip_manager.get_nn_text(caption_texts, caption_feats, img_feats)\n",
    "            best_caption = sorted_captions[0]\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Create a dictionary that maps the objects to the cosine sim.\n",
    "            object_embeddings = dict(zip(vocab_manager.object_list, object_feats))\n",
    "\n",
    "            # Create a list that contains the objects ordered by cosine sim.\n",
    "            embeddings_sorted = [object_embeddings[w] for w in sorted_obj_texts]\n",
    "\n",
    "            # Create a list to store the best matches\n",
    "            best_matches = [sorted_obj_texts[0]]\n",
    "\n",
    "            # Create an array to store the embeddings of the best matches\n",
    "            unique_embeddings = embeddings_sorted[0].reshape(-1, 1)\n",
    "\n",
    "            # Loop through the 100 best objects by cosine similarity\n",
    "            for i in range(1, 100):\n",
    "                # Obtain the maximum cosine similarity when comparing object i to the embeddings of the current best matches\n",
    "                max_cos_sim = (unique_embeddings.T @ embeddings_sorted[i]).max()\n",
    "                # If object i is different enough to the current best matches, add it to the best matches\n",
    "                if max_cos_sim < 0.7:\n",
    "                    print(f'{sorted_obj_texts[i]}: {unique_embeddings.T @ embeddings_sorted[i]}')\n",
    "                    unique_embeddings = np.concatenate([unique_embeddings, embeddings_sorted[i].reshape(-1, 1)], 1)\n",
    "                    best_matches.append(sorted_obj_texts[i])\n",
    "\n",
    "            # Looping through the best matches, consider each terms separately by splitting the commas and spaces.\n",
    "            data_list = []\n",
    "            for terms in best_matches:\n",
    "                for term_split in terms.split(', '):\n",
    "                    score = clip_manager.get_image_caption_score(term_split, img_feats)\n",
    "                    data_list.append({\n",
    "                        'term': term_split, 'score': score, 'context': terms\n",
    "                    })\n",
    "                    term_split_split = term_split.split(' ')\n",
    "                    if len(term_split_split) > 1:\n",
    "                        for term_split2 in term_split_split:\n",
    "                            score = clip_manager.get_image_caption_score(term_split2, img_feats)\n",
    "                            data_list.append({\n",
    "                                'term': term_split2, 'score': score, 'context': terms\n",
    "                            })\n",
    "\n",
    "            # Create a dataframe with the terms and scores and only keep the top term per context.\n",
    "            term_df = pd.DataFrame(data_list).sort_values('score', ascending=False).drop_duplicates('context').reset_index(drop=True)\n",
    "\n",
    "            # Prepare loop to find if additional terms can improve cosine similarity\n",
    "            best_terms_sorted = term_df['term'].tolist()\n",
    "            best_term = best_terms_sorted[0]\n",
    "            terms_to_check = list(set(best_terms_sorted[1:]))\n",
    "            best_cos_sim = term_df['score'].iloc[0]\n",
    "            terms_to_include = [best_term]\n",
    "\n",
    "            # Perform a loop to find if additional terms can improve the cosine similarity\n",
    "            n_iteration = 5\n",
    "            for iteration in range(n_iteration):\n",
    "                data_list = []\n",
    "                for term_to_test in terms_to_check:\n",
    "                    new_term = f\"{best_term} {term_to_test}\"\n",
    "                    score = clip_manager.get_image_caption_score(new_term, img_feats)\n",
    "                    data_list.append({\n",
    "                        'term': new_term, 'candidate': term_to_test, 'score': score\n",
    "                    })\n",
    "                combined_df = pd.DataFrame(data_list).sort_values('score', ascending=False)\n",
    "                if combined_df['score'].iloc[0] > best_cos_sim + 0.01:\n",
    "                    diff = combined_df['score'].iloc[0] - best_cos_sim\n",
    "                    print(f'term: {combined_df[\"candidate\"].iloc[0]}, diff: {diff}')\n",
    "                    best_cos_sim = combined_df['score'].iloc[0]\n",
    "                    terms_to_include.append(combined_df['candidate'].iloc[0])\n",
    "                    terms_to_check = combined_df['candidate'].tolist()[1:]\n",
    "                    best_term += f\" {combined_df['candidate'].iloc[0]}\"\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Generate 100 captions, order them and print out the best.\n",
    "            num_captions = 100\n",
    "            prompt = f'''Create a creative beautiful caption from this context:\n",
    "                \"This image is a {img_type}. There {ppl_result}.\n",
    "                The context is: {', '.join(terms_to_include)}.\n",
    "                A creative short caption I can generate to describe this image is:'''\n",
    "            model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "            caption_texts = flan_manager.generate_response([prompt] * num_captions, model_params)\n",
    "\n",
    "            # Zero-shot VLM: rank captions.\n",
    "            caption_feats = clip_manager.get_text_feats(caption_texts)\n",
    "            sorted_captions, caption_scores = clip_manager.get_nn_text(caption_texts, caption_feats, img_feats)\n",
    "            caption_score_map = dict(zip(sorted_captions, caption_scores))\n",
    "            best_caption = sorted_captions[0]\n",
    "            \n",
    "            \n",
    "        print(f'time taken {time.time()-start_time}')\n",
    "        cpt_feats = clip_manager.get_text_feats([best_caption]).flatten()\n",
    "            \n",
    "        return image_id, best_caption, img_feats, cpt_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cache/res.pickle'):\n",
    "    # Instantiate the Flan T5 manager\n",
    "    flan_manager = FlanT5Manager(version=\"google/flan-t5-xl\", use_api=False)\n",
    "\n",
    "    res = {}\n",
    "    embed_imgs = {}\n",
    "    embed_capt_res = {}\n",
    "\n",
    "    # N = len(os.listdir(imgs_folder))\n",
    "    N = 100\n",
    "    random_numbers = random.sample(range(len(os.listdir(imgs_folder))), N)\n",
    "\n",
    "    # for ix, file_name in enumerate(os.listdir(imgs_folder)[:N]):\n",
    "    for ix, file_name in enumerate(os.listdir(imgs_folder)):\n",
    "        \n",
    "        image_id, best_caption, img_feats, cpt_feats = caption_this_image(filename, imgs_folder, ix, random_numbers, image_manager, clip_manager, vocab_manager, flan_manager)\n",
    "        res[image_id] = [{\n",
    "            'image_id': image_id,\n",
    "            'id': image_id,\n",
    "            'caption': best_caption\n",
    "        }]\n",
    "        embed_imgs[image_id] = img_feats\n",
    "        embed_capt_res[image_id] = cpt_feats\n",
    "\n",
    "    # Saving the generated captions, image and generated caption embeddings\n",
    "    with open('cache/res.pickle', 'wb') as handle:\n",
    "        pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('cache/embed_imgs.pickle', 'wb') as handle:\n",
    "        pickle.dump(embed_imgs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('cache/embed_capt_res.pickle', 'wb') as handle:\n",
    "        pickle.dump(embed_capt_res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('cache/res.pickle', 'rb') as handle:\n",
    "        res = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1967d18",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the generated captions against the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44bc3d",
   "metadata": {},
   "source": [
    "#### Load the ground truth annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0daf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    lines = json.load(f)['annotations']\n",
    "gts = {}\n",
    "for item in lines:\n",
    "    if item['image_id'] not in gts:\n",
    "        gts[item['image_id']] = []\n",
    "    gts[item['image_id']].append({'image_id': item['image_id'], 'caption': item['caption']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0ff84",
   "metadata": {},
   "source": [
    "#### Compute the embeddings for the gt captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b42d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cache/embed_capt_gt.pickle'):\n",
    "    embed_capt_gt = {}\n",
    "    for img_id, list_of_capt_dict in gts.items():\n",
    "        list_of_captions = [capt_dict['caption'] for capt_dict in list_of_capt_dict]\n",
    "\n",
    "        # Dims of img_feats_gt: 5 x 768\n",
    "        img_feats_gt = clip_manager.get_text_feats(list_of_captions)\n",
    "\n",
    "        embed_capt_gt[img_id] = img_feats_gt\n",
    "\n",
    "    with open('cache/embed_capt_gt.pickle', 'wb') as handle:\n",
    "        pickle.dump(embed_capt_gt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30577e5e",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cap = {}\n",
    "evaluator = SocraticEvalCap(gts, res)\n",
    "\n",
    "# Rule-based metrics\n",
    "evaluator.evaluate_rulebased()\n",
    "eval_rulebased = {}\n",
    "for metric, score in evaluator.eval.items():\n",
    "    print(f'{metric}: {score:.3f}')\n",
    "    eval_rulebased[metric] = round(score, 5)\n",
    "eval_cap['rulebased'] = eval_rulebased\n",
    "\n",
    "# Embedding-based metric\n",
    "evaluator.evaluate_cossim()\n",
    "for source_caption, sim in evaluator.sims.items():\n",
    "    print(f'{source_caption}: avg = {sim[0]:.3f}, std = {sim[1]:.3f}')\n",
    "eval_cap['cossim'] = evaluator.sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305f506",
   "metadata": {},
   "source": [
    "#### Save the evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48481684",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_cap.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_cap, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885ba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979a633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc8f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988bb887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db459c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f15bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0af0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
