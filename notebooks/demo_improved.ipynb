{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b9511a",
   "metadata": {},
   "source": [
    "# SocraticFlanT5 - Improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74a311",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff717b4",
   "metadata": {},
   "source": [
    "In this notebook, we propose a new method to obtain image captions via the Socratic method.\n",
    "This is an improved pipeline that has for goal to generate similar or improved captions using open-source and free models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c510a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a939ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package loading\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "\n",
    "# Local imports\n",
    "from scripts.image_captioning import ClipManager, ImageManager, VocabManager, FlanT5Manager\n",
    "from scripts.image_captioning import LmPromptGenerator as pg\n",
    "from scripts.utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28928b4",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transformers seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd734f5",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to use\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8dc2e",
   "metadata": {},
   "source": [
    "## Class instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8664150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clip manager\n",
    "clip_manager = ClipManager(device)\n",
    "\n",
    "# Instantiate the image manager\n",
    "image_manager = ImageManager()\n",
    "\n",
    "# Instantiate the vocab manager\n",
    "vocab_manager = VocabManager()\n",
    "\n",
    "# Instantiate the Flan T5 manager\n",
    "flan_manager = FlanT5Manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f34fd",
   "metadata": {},
   "source": [
    "## Set image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455043c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = {\n",
    "    'Living room': 'demo_img.png',\n",
    "    'Monkey with gun': 'monkey_with_gun.jpg',\n",
    "    'Cute bear': 'cute_bear.jpg',\n",
    "    'Astronaut with beer': 'astronaut_with_beer.jpg'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cb7fc",
   "metadata": {},
   "source": [
    "## Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the place features\n",
    "place_feats = clip_manager.get_text_feats([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "\n",
    "# Calculate the object features\n",
    "object_feats = clip_manager.get_text_feats([f'Photo of a {o}.' for o in vocab_manager.object_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa36cd",
   "metadata": {},
   "source": [
    "## Load image and compute image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image.\n",
    "img_dic = {img_name: image_manager.load_image(img_path) for img_name, img_path in img_paths.items()}\n",
    "# Get image representation\n",
    "img_feat_dic = {img_name: clip_manager.get_img_feats(img) for img_name, img in img_dic.items()}\n",
    "# Show the image\n",
    "for img_name, img in img_dic.items():\n",
    "    print(f'{img_name}:')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d830216",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Image type classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393274a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify image type.\n",
    "img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "img_types_feats = clip_manager.get_text_feats([f'This is a {t}.' for t in img_types])\n",
    "\n",
    "# Create a dictionary to store the image types\n",
    "img_type_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_img_types, img_type_scores = clip_manager.get_nn_text(img_types, img_types_feats, img_feat)\n",
    "    img_type_dic[img_name] = sorted_img_types[0]\n",
    "    print(f'Image type for \"{img_name}\": {img_type_dic[img_name]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d03f57",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Number of people classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify number of people.\n",
    "ppl_texts = [\n",
    "    'are no people', 'is one person', 'are two people', 'are three people', 'are several people', 'are many people'\n",
    "]\n",
    "ppl_feats = clip_manager.get_text_feats([f'There {p} in this photo.' for p in ppl_texts])\n",
    "\n",
    "# Create a dictionary to store the number of people\n",
    "num_people_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_feats, img_feat)\n",
    "    num_people_dic[img_name] = sorted_ppl_texts[0]\n",
    "    print(f'Number of people in \"{img_name}\": {num_people_dic[img_name]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24e34c",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Image place classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify places.\n",
    "place_topk = 3\n",
    "\n",
    "# Create a dictionary to store the number of people\n",
    "location_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_places, places_scores = clip_manager.get_nn_text(vocab_manager.place_list, place_feats, img_feat)\n",
    "    location_dic[img_name] = sorted_places[0]\n",
    "    print(f'Location for {img_name}: {location_dic[img_name]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612ef9f",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Image object classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify objects.\n",
    "obj_topk = 10\n",
    "\n",
    "# Create a dictionary to store the similarity of each object with the images\n",
    "object_score_map = {}\n",
    "sorted_obj_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_obj_texts, obj_scores = clip_manager.get_nn_text(vocab_manager.object_list, object_feats, img_feat)\n",
    "    object_score_map[img_name] = dict(zip(sorted_obj_texts, obj_scores))\n",
    "    sorted_obj_dic[img_name] = sorted_obj_texts\n",
    "    object_list = ''\n",
    "    for i in range(obj_topk):\n",
    "        object_list += f'{sorted_obj_texts[i]}, '\n",
    "    object_list = object_list[:-2]\n",
    "    print(f'Top 10 objects recognised for \"{img_name}\":\\n{object_list}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c2109",
   "metadata": {},
   "source": [
    "## Finding both relevant and different objects using the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps the objects to the cosine sim.\n",
    "object_embeddings = dict(zip(vocab_manager.object_list, object_feats))\n",
    "\n",
    "# Create a dictionary to store the terms to include\n",
    "terms_to_include = {}\n",
    "\n",
    "for img_name, sorted_obj_texts in sorted_obj_dic.items():\n",
    "\n",
    "    # Create a list that contains the objects ordered by cosine sim.\n",
    "    embeddings_sorted = [object_embeddings[w] for w in sorted_obj_texts]\n",
    "\n",
    "    # Create a list to store the best matches\n",
    "    best_matches = [sorted_obj_texts[0]]\n",
    "\n",
    "    # Create an array to store the embeddings of the best matches\n",
    "    unique_embeddings = embeddings_sorted[0].reshape(-1, 1)\n",
    "\n",
    "    # Loop through the 100 best objects by cosine similarity\n",
    "    for i in range(1, 100):\n",
    "        # Obtain the maximum cosine similarity when comparing object i to the embeddings of the current best matches\n",
    "        max_cos_sim = (unique_embeddings.T @ embeddings_sorted[i]).max()\n",
    "        # If object i is different enough to the current best matches, add it to the best matches\n",
    "        if max_cos_sim < 0.7:\n",
    "            unique_embeddings = np.concatenate([unique_embeddings, embeddings_sorted[i].reshape(-1, 1)], 1)\n",
    "            best_matches.append(sorted_obj_texts[i])\n",
    "\n",
    "    # Looping through the best matches, consider each terms separately by splitting the commas and spaces.\n",
    "    data_list = []\n",
    "    for terms in best_matches:\n",
    "        for term_split in terms.split(', '):\n",
    "            score = clip_manager.get_image_caption_score(term_split, img_feat_dic[img_name])\n",
    "            data_list.append({\n",
    "                'term': term_split, 'score': score, 'context': terms\n",
    "            })\n",
    "            term_split_split = term_split.split(' ')\n",
    "            if len(term_split_split) > 1:\n",
    "                for term_split2 in term_split_split:\n",
    "                    score = clip_manager.get_image_caption_score(term_split2, img_feat_dic[img_name])\n",
    "                    data_list.append({\n",
    "                        'term': term_split2, 'score': score, 'context': terms\n",
    "                    })\n",
    "\n",
    "    # Create a dataframe with the terms and scores and only keep the top term per context.\n",
    "    term_df = pd.DataFrame(data_list).sort_values('score', ascending=False).drop_duplicates('context').reset_index(drop=True)\n",
    "\n",
    "    # Prepare loop to find if additional terms can improve cosine similarity\n",
    "    best_terms_sorted = term_df['term'].tolist()\n",
    "    best_term = best_terms_sorted[0]\n",
    "    terms_to_check = list(set(best_terms_sorted[1:]))\n",
    "    best_cos_sim = term_df['score'].iloc[0]\n",
    "    terms_to_include[img_name] = [best_term]\n",
    "\n",
    "    # Perform a loop to find if additional terms can improve the cosine similarity\n",
    "    n_iteration = 5\n",
    "    for iteration in range(n_iteration):\n",
    "        data_list = []\n",
    "        for term_to_test in terms_to_check:\n",
    "            new_term = f\"{best_term} {term_to_test}\"\n",
    "            score = clip_manager.get_image_caption_score(new_term, img_feat_dic[img_name])\n",
    "            data_list.append({\n",
    "                'term': new_term, 'candidate': term_to_test, 'score': score\n",
    "            })\n",
    "        combined_df = pd.DataFrame(data_list).sort_values('score', ascending=False)\n",
    "        if combined_df['score'].iloc[0] > best_cos_sim + 0.01:\n",
    "            best_cos_sim = combined_df['score'].iloc[0]\n",
    "            terms_to_include[img_name].append(combined_df['candidate'].iloc[0])\n",
    "            terms_to_check = combined_df['candidate'].tolist()[1:]\n",
    "            best_term += f\" {combined_df['candidate'].iloc[0]}\"\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee1a38",
   "metadata": {},
   "source": [
    "## Zero shot LM - Caption generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ce0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 50 captions, order them and print out the best.\n",
    "num_captions = 50\n",
    "\n",
    "# Set LM params\n",
    "model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "\n",
    "# Create dictionaries to store the outputs\n",
    "prompt_dic = {}\n",
    "sorted_caption_map = {}\n",
    "caption_score_map = {}\n",
    "\n",
    "for img_name in img_dic:\n",
    "    # Create the prompt for the language model\n",
    "    prompt_dic[img_name] = pg.create_improved_lm_prompt(\n",
    "        img_type_dic[img_name], num_people_dic[img_name], terms_to_include[img_name]\n",
    "    )\n",
    "\n",
    "    # Generate the caption using the language model\n",
    "    caption_texts = flan_manager.generate_response(num_captions * [prompt_dic[img_name]], model_params)\n",
    "\n",
    "    # Zero-shot VLM: rank captions.\n",
    "    caption_feats = clip_manager.get_text_feats(caption_texts)\n",
    "    sorted_captions, caption_scores = clip_manager.get_nn_text(caption_texts, caption_feats, img_feat_dic[img_name])\n",
    "    sorted_caption_map[img_name] = sorted_captions\n",
    "    caption_score_map[img_name] = dict(zip(sorted_captions, caption_scores))\n",
    "    print(f'Best generate caption for \"{img_name}\": \"{sorted_captions[0]}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a78d8d",
   "metadata": {},
   "source": [
    "## Saving the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8967d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for img_name in img_dic:\n",
    "    generated_caption = sorted_caption_map[img_name][0]\n",
    "    data_list.append({\n",
    "        'image_name': img_name,\n",
    "        'image_path': img_paths[img_name],\n",
    "        'generated_caption': generated_caption,\n",
    "        'cosine_similarity': caption_score_map[img_name][generated_caption]\n",
    "    })\n",
    "pd.DataFrame(data_list).to_csv(f'baseline_outputs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
