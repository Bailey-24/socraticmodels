{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655ff268",
   "metadata": {},
   "source": [
    "# SocraticFlanT5 - Caption Generation (baseline) | DL2 Project, May 2023\n",
    "---\n",
    "\n",
    "This notebook downloads the images from the validation split of the [MS COCO Dataset (2017 version)](https://cocodataset.org/#download) and the corresponding ground-truth captions and generates captions based on the Socratic model pipeline outlined below. The caption will be generated by the baseline approach:\n",
    "* <span style=\"color:#006400\">**Baseline**</span>: a Socratic model based on the work by [Zeng et al. (2022)](https://socraticmodels.github.io/) where GPT-3 is replaced by [FLAN-T5-xl](https://huggingface.co/docs/transformers/model_doc/flan-t5). \n",
    "\n",
    "In other words, the goal of this jupyter notebook is to reproduce the Socratic Models paper with the Flan-T5 model. This provides a baseline for us to build upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e331e",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "If you haven't done so already, please activate the corresponding environment by running in the terminal: `conda env create -f environment.yml`. Then type `conda activate socratic`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1503d1d",
   "metadata": {},
   "source": [
    "### Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11989003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package loading\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Local imports\n",
    "from image_captioning import ClipManager, ImageManager, VocabManager, FlanT5Manager, COCOManager\n",
    "from image_captioning import LmPromptGenerator as pg\n",
    "from utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d341a53",
   "metadata": {},
   "source": [
    "### Set seeds for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HuggingFace seed\n",
    "set_seed(42)\n",
    "\n",
    "# Set seed for 100 random images of the MS COCO validation split\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada75e",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the MS COCO images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898c7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_folder = 'imgs/val2017/'\n",
    "annotation_file = 'annotations/annotations/captions_val2017.json'\n",
    "\n",
    "coco_manager = COCOManager()\n",
    "coco_manager.download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c54da",
   "metadata": {},
   "source": [
    "## Step 2: Generating the captions via the Socratic pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aab5ad",
   "metadata": {},
   "source": [
    "### Set the device and instantiate managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bcac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 0.0s!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1238bdb27c184ecabc4bc5ff6d295e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the device to use\n",
    "device = get_device()\n",
    "\n",
    "# Instantiate the clip manager\n",
    "clip_manager = ClipManager(device)\n",
    "\n",
    "# Instantiate the image manager\n",
    "image_manager = ImageManager()\n",
    "\n",
    "# Instantiate the vocab manager\n",
    "vocab_manager = VocabManager()\n",
    "\n",
    "# Instantiate the Flan T5 manager\n",
    "flan_manager = FlanT5Manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0d924",
   "metadata": {},
   "source": [
    "### Compute place and object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372c21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the place features\n",
    "if not os.path.exists('cache/place_feats.npy'):\n",
    "    # Calculate the place features\n",
    "    place_feats = clip_manager.get_text_feats([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "    np.save('cache/place_feats.npy', place_feats)\n",
    "else:\n",
    "    place_feats = np.load('cache/place_feats.npy')\n",
    "\n",
    "# Calculate the object features\n",
    "if not os.path.exists('cache/object_feats.npy'):\n",
    "    # Calculate the object features\n",
    "    object_feats = clip_manager.get_text_feats([f'Photo of a {o}.' for o in vocab_manager.object_list])\n",
    "    np.save('cache/object_feats.npy', object_feats)\n",
    "else:\n",
    "    object_feats = np.load('cache/object_feats.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59985dbb",
   "metadata": {},
   "source": [
    "### Load images and compute image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d44faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'baseline'\n",
    "\n",
    "img_dic = {}\n",
    "img_feat_dic = {}\n",
    "img_paths = {}\n",
    "# if not os.path.exists(f'{approach}_outputs.csv'):\n",
    "    # N = len(os.listdir(imgs_folder))\n",
    "    # N = 100\n",
    "N = 100\n",
    "random_numbers = random.sample(range(len(os.listdir(imgs_folder))), N)\n",
    "\n",
    "# for ix, file_name in enumerate(os.listdir(imgs_folder)[:N]):\n",
    "for ix, file_name in enumerate(os.listdir(imgs_folder)):\n",
    "     # Consider only image files that are part of the random sample\n",
    "    if file_name.endswith(\".jpg\") and ix in random_numbers:  \n",
    "        # Getting image id\n",
    "        file_name_strip = file_name.strip('.jpg')\n",
    "        match = re.search('^0+', file_name_strip)\n",
    "        sequence = match.group(0)\n",
    "        image_id = int(file_name_strip[len(sequence):])\n",
    "\n",
    "        img_path = os.path.join(imgs_folder, file_name)\n",
    "        img = image_manager.load_image(img_path)\n",
    "        img_feats = clip_manager.get_img_feats(img)\n",
    "        img_feats = img_feats.flatten()\n",
    "        img_paths[image_id] = file_name\n",
    "\n",
    "        img_dic[image_id] = img\n",
    "        img_feat_dic[image_id] = img_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13299759",
   "metadata": {},
   "source": [
    "### Zero-shot VLM (CLIP)\n",
    "We zero-shot prompt CLIP to produce various inferences of an iage, such as image type or the number of people in an image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6632d",
   "metadata": {},
   "source": [
    "#### Classify image type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be894f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "img_types_feats = clip_manager.get_text_feats([f'This is a {t}.' for t in img_types])\n",
    "\n",
    "# Create a dictionary to store the image types\n",
    "img_type_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_img_types, img_type_scores = clip_manager.get_nn_text(img_types, img_types_feats, img_feat)\n",
    "    img_type_dic[img_name] = sorted_img_types[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3867d",
   "metadata": {},
   "source": [
    "#### Classify number of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_texts_bool = ['no people', 'people']\n",
    "ppl_feats_bool = clip_manager.get_text_feats([f'There are {p} in this photo.' for p in ppl_texts_bool])\n",
    "\n",
    "ppl_texts_mult = ['is one person', 'are two people', 'are three people', 'are several people', 'are many people']\n",
    "ppl_feats_mult = clip_manager.get_text_feats([f'There {p} in this photo.' for p in ppl_texts_mult])\n",
    "\n",
    "# Create a dictionary to store the number of people\n",
    "num_people_dic = {}\n",
    "\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts_bool, ppl_feats_bool, img_feat)\n",
    "    ppl_result = sorted_ppl_texts[0]\n",
    "    if ppl_result == 'people':\n",
    "        sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts_mult, ppl_feats_mult, img_feat)\n",
    "        ppl_result = sorted_ppl_texts[0]\n",
    "    else:\n",
    "        ppl_result = f'are {ppl_result}'\n",
    "\n",
    "    num_people_dic[img_name] = ppl_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4549f",
   "metadata": {},
   "source": [
    "#### Classify image place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_topk = 3\n",
    "\n",
    "# Create a dictionary to store the number of people\n",
    "location_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_places, places_scores = clip_manager.get_nn_text(vocab_manager.place_list, place_feats, img_feat)\n",
    "    location_dic[img_name] = sorted_places[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf872a1",
   "metadata": {},
   "source": [
    "#### Classify image object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_topk = 10\n",
    "\n",
    "# Create a dictionary to store the similarity of each object with the images\n",
    "obj_list_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_obj_texts, obj_scores = clip_manager.get_nn_text(vocab_manager.object_list, object_feats, img_feat)\n",
    "    object_list = ''\n",
    "    for i in range(obj_topk):\n",
    "        object_list += f'{sorted_obj_texts[i]}, '\n",
    "    object_list = object_list[:-2]\n",
    "    obj_list_dic[img_name] = object_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e564fd3",
   "metadata": {},
   "source": [
    "#### Generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_captions = 50\n",
    "\n",
    "# Set LM params\n",
    "model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "\n",
    "# Create dictionaries to store the outputs\n",
    "prompt_dic = {}\n",
    "sorted_caption_map = {}\n",
    "caption_score_map = {}\n",
    "\n",
    "for img_name in img_dic:\n",
    "    # Create the prompt for the language model\n",
    "    prompt_dic[img_name] = pg.create_baseline_lm_prompt(\n",
    "        img_type_dic[img_name], num_people_dic[img_name], location_dic[img_name], obj_list_dic[img_name]\n",
    "    )\n",
    "\n",
    "    # Generate the caption using the language model\n",
    "    caption_texts = flan_manager.generate_response(num_captions * [prompt_dic[img_name]], model_params)\n",
    "\n",
    "    # Zero-shot VLM: rank captions.\n",
    "    caption_feats = clip_manager.get_text_feats(caption_texts)\n",
    "    sorted_captions, caption_scores = clip_manager.get_nn_text(caption_texts, caption_feats, img_feat_dic[img_name])\n",
    "    sorted_caption_map[img_name] = sorted_captions\n",
    "    caption_score_map[img_name] = dict(zip(sorted_captions, caption_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260f29b",
   "metadata": {},
   "source": [
    "### Save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for img_name in img_dic:\n",
    "    generated_caption = sorted_caption_map[img_name][0]\n",
    "    data_list.append({\n",
    "        'image_name': img_name,\n",
    "        'image_path': img_paths[img_name],\n",
    "        'generated_caption': generated_caption,\n",
    "        'cosine_similarity': caption_score_map[img_name][generated_caption]\n",
    "    })\n",
    "pd.DataFrame(data_list).to_csv(f'{approach}_outputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0af0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
