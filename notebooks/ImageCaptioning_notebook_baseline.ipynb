{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00344f54",
   "metadata": {},
   "source": [
    "# Image Captioning - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58295f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# !conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9910147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package loading\n",
    "import os\n",
    "import requests\n",
    "import clip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from profanity_filter import ProfanityFilter\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from scripts.utils import print_time_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec8f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageManager:\n",
    "    def __init__(self):\n",
    "        self.download_data()\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data():\n",
    "        # Download test image\n",
    "        fname = 'demo_img.png'\n",
    "        if not os.path.exists(fname):\n",
    "            img_url = \"https://github.com/rmokady/CLIP_prefix_caption/raw/main/Images/COCO_val2014_000000165547.jpg\"\n",
    "            with open(fname, 'wb') as f:\n",
    "                f.write(requests.get(img_url).content)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image(image_path):\n",
    "        return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e042a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabManager:\n",
    "    def __init__(self):\n",
    "        self.download_data()\n",
    "        self.place_list = self.load_places()\n",
    "        self.object_list = self.load_objects(remove_profanity=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data():\n",
    "        # Download scene categories from Places365.\n",
    "        if not os.path.exists('categories_places365.txt'):\n",
    "            url = \"https://raw.githubusercontent.com/zhoubolei/places_devkit/master/categories_places365.txt\"\n",
    "            response = requests.get(url)\n",
    "            with open(\"categories_places365.txt\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        # Download object categories from Tencent ML Images.\n",
    "        if not os.path.exists('dictionary_and_semantic_hierarchy.txt'):\n",
    "            url = \"https://raw.githubusercontent.com/Tencent/tencent-ml-images/master/data/dictionary_and_semantic_hierarchy.txt\"\n",
    "            response = requests.get(url)\n",
    "            with open(\"dictionary_and_semantic_hierarchy.txt\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "    @staticmethod\n",
    "    @print_time_dec\n",
    "    def load_places():\n",
    "        place_categories = np.loadtxt('categories_places365.txt', dtype=str)\n",
    "        place_texts = []\n",
    "        for place in place_categories[:, 0]:\n",
    "            place = place.split('/')[2:]\n",
    "            if len(place) > 1:\n",
    "                place = place[1] + ' ' + place[0]\n",
    "            else:\n",
    "                place = place[0]\n",
    "            place = place.replace('_', ' ')\n",
    "            place_texts.append(place)\n",
    "        return place_texts\n",
    "\n",
    "    @print_time_dec\n",
    "    def load_objects(self, remove_profanity=False):\n",
    "        with open('dictionary_and_semantic_hierarchy.txt') as fid:\n",
    "            object_categories = fid.readlines()\n",
    "        object_texts = []\n",
    "        pf = ProfanityFilter()\n",
    "        for object_text in object_categories[1:]:\n",
    "            object_text = object_text.strip()\n",
    "            object_text = object_text.split('\\t')[3]\n",
    "            if remove_profanity:\n",
    "                safe_list = ''\n",
    "                for variant in object_text.split(','):\n",
    "                    text = variant.strip()\n",
    "                    if pf.is_clean(text):\n",
    "                        safe_list += f'{text}, '\n",
    "\n",
    "                safe_list = safe_list[:-2]\n",
    "                if len(safe_list) > 0:\n",
    "                    object_texts.append(safe_list)\n",
    "            else:\n",
    "                object_texts.append(object_text)\n",
    "        return [o for o in list(set(object_texts)) if o not in self.place_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00025c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipManager:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.feat_dim_map = {\n",
    "            'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768, 'RN50x64': 1024, 'ViT-B/32': 512,\n",
    "            'ViT-B/16': 512,'ViT-L/14': 768\n",
    "        }\n",
    "        self.version = \"ViT-L/14\"\n",
    "        self.feat_dim = self.feat_dim_map[self.version]\n",
    "        self.model, self.preprocess = clip.load(self.version)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @print_time_dec\n",
    "    def get_text_feats(self, in_text, batch_size=64):\n",
    "        text_tokens = clip.tokenize(in_text).to(self.device)\n",
    "        text_id = 0\n",
    "        text_feats = np.zeros((len(in_text), self.feat_dim), dtype=np.float32)\n",
    "        while text_id < len(text_tokens):  # Batched inference.\n",
    "            batch_size = min(len(in_text) - text_id, batch_size)\n",
    "            text_batch = text_tokens[text_id:text_id + batch_size]\n",
    "            with torch.no_grad():\n",
    "                batch_feats = self.model.encode_text(text_batch).float()\n",
    "            batch_feats /= batch_feats.norm(dim=-1, keepdim=True)\n",
    "            batch_feats = np.float32(batch_feats.cpu())\n",
    "            text_feats[text_id:text_id + batch_size, :] = batch_feats\n",
    "            text_id += batch_size\n",
    "        return text_feats\n",
    "\n",
    "    def get_img_feats(self, img):\n",
    "        img_pil = Image.fromarray(np.uint8(img))\n",
    "        img_in = self.preprocess(img_pil)[None, ...]\n",
    "        with torch.no_grad():\n",
    "            img_feats = self.model.encode_image(img_in.to(self.device)).float()\n",
    "        img_feats /= img_feats.norm(dim=-1, keepdim=True)\n",
    "        img_feats = np.float32(img_feats.cpu())\n",
    "        return img_feats\n",
    "\n",
    "    @staticmethod\n",
    "    def get_nn_text(raw_texts, text_feats, img_feats):\n",
    "        scores = text_feats @ img_feats.T\n",
    "        scores = scores.squeeze()\n",
    "        high_to_low_ids = np.argsort(scores).squeeze()[::-1]\n",
    "        high_to_low_texts = [raw_texts[i] for i in high_to_low_ids]\n",
    "        high_to_low_scores = np.sort(scores).squeeze()[::-1]\n",
    "        return high_to_low_texts, high_to_low_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b38c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlanT5Manager:\n",
    "    def __init__(self, version=\"google/flan-t5-xl\"):\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(version)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(version)\n",
    "\n",
    "    def generate_response(self, prompt, model_params=None):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**inputs, **model_params)\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e954ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "    return np.sum([int(np.prod(p.shape)) for p in model.parameters()])\n",
    "\n",
    "\n",
    "def print_clip_info(model):\n",
    "    print(\"Model parameters (total):\", num_params(model))\n",
    "    print(\"Model parameters (image encoder):\", num_params(model.visual))\n",
    "    print(\"Model parameters (text encoder):\", num_params(model.token_embedding) + num_params(model.transformer))\n",
    "    print(\"Input image resolution:\", model.visual.input_resolution)\n",
    "    print(\"Context length:\", model.context_length)\n",
    "    print(\"Vocab size:\", model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cce611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(img_path='demo_img.png', verbose=True):\n",
    "\n",
    "    # Set the device to use\n",
    "    if getattr(torch, 'has_mps', False):\n",
    "        device = 'mps'\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'gpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    # Instantiate the clip manager\n",
    "    clip_manager = ClipManager(device)\n",
    "\n",
    "    # Instantiate the image manager\n",
    "    image_manager = ImageManager()\n",
    "\n",
    "    # Instantiate the vocab manager\n",
    "    vocab_manager = VocabManager()\n",
    "\n",
    "    # Instantiate the Flan T5 manager\n",
    "    flan_manager = FlanT5Manager()\n",
    "\n",
    "    # Print out clip model info\n",
    "    print_clip_info(clip_manager.model)\n",
    "\n",
    "    # Calculate the place features\n",
    "    place_feats = clip_manager.get_text_feats([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "\n",
    "    # Calculate the object features\n",
    "    object_feats = clip_manager.get_text_feats([f'Photo of a {o}.' for o in vocab_manager.object_list])\n",
    "\n",
    "    # Load image.\n",
    "    img = image_manager.load_image(img_path)\n",
    "    img_feats = clip_manager.get_img_feats(img)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # Zero-shot VLM: classify image type.\n",
    "    img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "    img_types_feats = clip_manager.get_text_feats([f'This is a {t}.' for t in img_types])\n",
    "    sorted_img_types, img_type_scores = clip_manager.get_nn_text(img_types, img_types_feats, img_feats)\n",
    "    img_type = sorted_img_types[0]\n",
    "\n",
    "    # Zero-shot VLM: classify number of people.\n",
    "    ppl_texts = ['no people', 'people']\n",
    "    ppl_feats = clip_manager.get_text_feats([f'There are {p} in this photo.' for p in ppl_texts])\n",
    "    sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "    ppl_result = sorted_ppl_texts[0]\n",
    "    if ppl_result == 'people':\n",
    "        ppl_texts = ['is one person', 'are two people', 'are three people', 'are several people', 'are many people']\n",
    "        ppl_feats = clip_manager.get_text_feats([f'There {p} in this photo.' for p in ppl_texts])\n",
    "        sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "        ppl_result = sorted_ppl_texts[0]\n",
    "    else:\n",
    "        ppl_result = f'are {ppl_result}'\n",
    "\n",
    "    # Zero-shot VLM: classify places.\n",
    "    place_topk = 3\n",
    "    sorted_places, places_scores = clip_manager.get_nn_text(vocab_manager.place_list, place_feats, img_feats)\n",
    "\n",
    "    # Zero-shot VLM: classify objects.\n",
    "    obj_topk = 10\n",
    "    sorted_obj_texts, obj_scores = clip_manager.get_nn_text(vocab_manager.object_list, object_feats, img_feats)\n",
    "    object_list = ''\n",
    "    for i in range(obj_topk):\n",
    "        object_list += f'{sorted_obj_texts[i]}, '\n",
    "    object_list = object_list[:-2]\n",
    "\n",
    "    # Zero-shot LM: generate captions.\n",
    "    num_captions = 10\n",
    "    prompt = f'''I am an intelligent image captioning bot.\n",
    "    This image is a {img_type}. There {ppl_result}.\n",
    "    I think this photo was taken at a {sorted_places[0]}, {sorted_places[1]}, or {sorted_places[2]}.\n",
    "    I think there might be a {object_list} in this {img_type}.\n",
    "    A creative short caption I can generate to describe this image is:'''\n",
    "\n",
    "    model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "    caption_texts = [flan_manager.generate_response(prompt, model_params) for _ in range(num_captions)]\n",
    "\n",
    "    # Zero-shot VLM: rank captions.\n",
    "    caption_feats = clip_manager.get_text_feats(caption_texts)\n",
    "    sorted_captions, caption_scores = clip_manager.get_nn_text(caption_texts, caption_feats, img_feats)\n",
    "    print(f'{sorted_captions[0]}\\n')\n",
    "\n",
    "    if verbose:\n",
    "        print(f'VLM: This image is a:')\n",
    "        for img_type, score in zip(sorted_img_types, img_type_scores):\n",
    "            print(f'{score:.4f} {img_type}')\n",
    "\n",
    "        print(f'\\nVLM: There:')\n",
    "        for ppl_text, score in zip(sorted_ppl_texts, ppl_scores):\n",
    "            print(f'{score:.4f} {ppl_text}')\n",
    "\n",
    "        print(f'\\nVLM: I think this photo was taken at a:')\n",
    "        for place, score in zip(sorted_places[:place_topk], places_scores[:place_topk]):\n",
    "            print(f'{score:.4f} {place}')\n",
    "\n",
    "        print(f'\\nVLM: I think there might be a:')\n",
    "        for obj_text, score in zip(sorted_obj_texts[:obj_topk], obj_scores[:obj_topk]):\n",
    "            print(f'{score:.4f} {obj_text}')\n",
    "\n",
    "        print(f'\\nLM generated captions ranked by VLM scores:')\n",
    "        for caption, score in zip(sorted_captions, caption_scores):\n",
    "            print(f'{score:.4f} {caption}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6faf8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 93.7s!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6eeef07b7843ea888399546ace65ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 21\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(img_path, verbose)\u001B[0m\n\u001B[1;32m     18\u001B[0m vocab_manager \u001B[38;5;241m=\u001B[39m VocabManager()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Instantiate the Flan T5 manager\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m flan_manager \u001B[38;5;241m=\u001B[39m \u001B[43mFlanT5Manager\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Print out clip model info\u001B[39;00m\n\u001B[1;32m     24\u001B[0m print_clip_info(clip_manager\u001B[38;5;241m.\u001B[39mmodel)\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36mFlanT5Manager.__init__\u001B[0;34m(self, version)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle/flan-t5-xl\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForSeq2SeqLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(version)\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    470\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 471\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    477\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:2795\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   2785\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2786\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   2788\u001B[0m     (\n\u001B[1;32m   2789\u001B[0m         model,\n\u001B[1;32m   2790\u001B[0m         missing_keys,\n\u001B[1;32m   2791\u001B[0m         unexpected_keys,\n\u001B[1;32m   2792\u001B[0m         mismatched_keys,\n\u001B[1;32m   2793\u001B[0m         offload_index,\n\u001B[1;32m   2794\u001B[0m         error_msgs,\n\u001B[0;32m-> 2795\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2796\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2797\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2798\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# XXX: rename?\u001B[39;49;00m\n\u001B[1;32m   2799\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2800\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2801\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2802\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2803\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2804\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2805\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2806\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2807\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2808\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2809\u001B[0m \u001B[43m        \u001B[49m\u001B[43mload_in_8bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_in_8bit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2810\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2811\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2813\u001B[0m model\u001B[38;5;241m.\u001B[39mis_loaded_in_8bit \u001B[38;5;241m=\u001B[39m load_in_8bit\n\u001B[1;32m   2815\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:3141\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001B[0m\n\u001B[1;32m   3139\u001B[0m     error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_error_msgs\n\u001B[1;32m   3140\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3141\u001B[0m     error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43m_load_state_dict_into_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_prefix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3143\u001B[0m \u001B[38;5;66;03m# force memory release\u001B[39;00m\n\u001B[1;32m   3144\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m state_dict\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:529\u001B[0m, in \u001B[0;36m_load_state_dict_into_model\u001B[0;34m(model_to_load, state_dict, start_prefix)\u001B[0m\n\u001B[1;32m    526\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    527\u001B[0m             load(child, state_dict, prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 529\u001B[0m \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart_prefix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001B[39;00m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;66;03m# it's safe to delete it.\u001B[39;00m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m state_dict\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:527\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[0;34m(module, state_dict, prefix)\u001B[0m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 527\u001B[0m         \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:527\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[0;34m(module, state_dict, prefix)\u001B[0m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 527\u001B[0m         \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 527 (4 times)]\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:527\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[0;34m(module, state_dict, prefix)\u001B[0m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 527\u001B[0m         \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/modeling_utils.py:523\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[0;34m(module, state_dict, prefix)\u001B[0m\n\u001B[1;32m    521\u001B[0m                     module\u001B[38;5;241m.\u001B[39m_load_from_state_dict(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 523\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_from_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/newdl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1942\u001B[0m, in \u001B[0;36mModule._load_from_state_dict\u001B[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001B[0m\n\u001B[1;32m   1940\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1941\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m-> 1942\u001B[0m         \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy_\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_param\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1943\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[1;32m   1944\u001B[0m     error_msgs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhile copying the parameter named \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1945\u001B[0m                       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose dimensions in the model are \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1946\u001B[0m                       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose dimensions in the checkpoint are \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1947\u001B[0m                       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124man exception occurred : \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1948\u001B[0m                       \u001B[38;5;241m.\u001B[39mformat(key, param\u001B[38;5;241m.\u001B[39msize(), input_param\u001B[38;5;241m.\u001B[39msize(), ex\u001B[38;5;241m.\u001B[39margs))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201eae32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
