{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b9511a",
   "metadata": {},
   "source": [
    "# SocraticFlanT5 - Exploring the use of an additional vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74a311",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff717b4",
   "metadata": {},
   "source": [
    "The goal of this jupyter notebook is to reproduce the Socratic Models paper with the FlanT5 model.\n",
    "Here we simply reimplement the pipeline used by A. Zeng et al. as closely as possible.\n",
    "This provides a baseline for us to build upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c510a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a939ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package loading\n",
    "import os\n",
    "import requests\n",
    "import clip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from profanity_filter import ProfanityFilter\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from utils import print_time_dec\n",
    "import pandas as pd\n",
    "\n",
    "# Local imports\n",
    "from image_captioning import ClipManager, ImageManager, VocabManager, FlanT5Manager, print_clip_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd734f5",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d35df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to use\n",
    "if getattr(torch, 'has_mps', False):\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'gpu'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8dc2e",
   "metadata": {},
   "source": [
    "## Class instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8664150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the clip manager\n",
    "clip_manager = ClipManager(device)\n",
    "\n",
    "# Instantiate the image manager\n",
    "image_manager = ImageManager()\n",
    "\n",
    "# Instantiate the vocab manager\n",
    "vocab_manager = VocabManager()\n",
    "\n",
    "# Instantiate the Flan T5 manager\n",
    "flan_manager = FlanT5Manager()\n",
    "\n",
    "# Print out clip model info\n",
    "print_clip_info(clip_manager.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f34fd",
   "metadata": {},
   "source": [
    "## Set image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455043c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'monkey_with_gun.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ebd0f",
   "metadata": {},
   "source": [
    "## CLIP model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd49bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out clip model info\n",
    "print_clip_info(clip_manager.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cb7fc",
   "metadata": {},
   "source": [
    "## Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the place features\n",
    "place_feats = clip_manager.get_text_feats([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "\n",
    "# Calculate the object features\n",
    "object_feats = clip_manager.get_text_feats([f'Photo of a {o}.' for o in vocab_manager.object_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa36cd",
   "metadata": {},
   "source": [
    "## Load image and compute image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image.\n",
    "img = image_manager.load_image(img_path)\n",
    "# Get image representation\n",
    "img_feats = clip_manager.get_img_feats(img)\n",
    "# Show the image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d830216",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Image type classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393274a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify image type.\n",
    "img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "img_types_feats = clip_manager.get_text_feats([f'This is a {t}.' for t in img_types])\n",
    "sorted_img_types, img_type_scores = clip_manager.get_nn_text(img_types, img_types_feats, img_feats)\n",
    "img_type = sorted_img_types[0]\n",
    "print(f'This is a {img_type}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d03f57",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Number of people classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify number of people.\n",
    "ppl_texts = [\n",
    "    'are no people', 'is one person', 'are two people', 'are three people', 'are several people', 'are many people'\n",
    "]\n",
    "ppl_feats = clip_manager.get_text_feats([f'There {p} in this photo.' for p in ppl_texts])\n",
    "sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_feats, img_feats)\n",
    "ppl_result = sorted_ppl_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24e34c",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Image place classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify places.\n",
    "place_topk = 3\n",
    "sorted_places, places_scores = clip_manager.get_nn_text(vocab_manager.place_list, place_feats, img_feats)\n",
    "print(f'Location: {sorted_places[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612ef9f",
   "metadata": {},
   "source": [
    "## Zero shot VLM - Image object classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot VLM: classify objects.\n",
    "obj_topk = 10\n",
    "sorted_obj_texts, obj_scores = clip_manager.get_nn_text(vocab_manager.object_list, object_feats, img_feats)\n",
    "object_list = ''\n",
    "for i in range(obj_topk):\n",
    "    object_list += f'{sorted_obj_texts[i]}, '\n",
    "object_list = object_list[:-2]\n",
    "print(f'Top 10 objects recognized: \\n{sorted_obj_texts[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de80ac",
   "metadata": {},
   "source": [
    "## Exploring an additional vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb00f7da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Text file available here: https://github.com/nizarhabash1/catvar/blob/master/English-Morph.txt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m english_vocab \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish-Morph.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m active_verbs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish-Morph.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Text file available here: https://github.com/nizarhabash1/catvar/blob/master/English-Morph.txt\n",
    "\n",
    "english_vocab = pd.read_csv('English-Morph.txt', sep=' ')\n",
    "\n",
    "active_verbs = []\n",
    "with open('English-Morph.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for l in lines:\n",
    "        for w in l.split('\\t'):\n",
    "            if len(w) > 4 and w.endswith('ing'):\n",
    "                active_verbs.append(w)\n",
    "\n",
    "active_verbs = list(set(active_verbs))\n",
    "\n",
    "active_verbs_fea = clip_manager.get_text_feats(active_verbs)\n",
    "active_verbs_texts, active_verbs_scores = clip_manager.get_nn_text(active_verbs, active_verbs_fea, img_feats)\n",
    "\n",
    "active_verb_map = dict(zip(active_verbs_texts, active_verbs_scores))\n",
    "\n",
    "if len(terms_to_include) > 1:\n",
    "    data_list = []\n",
    "    for v in active_verbs_texts[:100]:\n",
    "\n",
    "        test_term = f'{terms_to_include[0]} {v} {terms_to_include[1]}'\n",
    "\n",
    "        score = clip_manager.get_image_caption_score(test_term, img_feats)\n",
    "\n",
    "        data_list.append({'verb': v, 'new_term': test_term, 'score': score})\n",
    "\n",
    "    verb_df = pd.DataFrame(data_list).sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee603ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in verb_df['verb'].iloc[:10]:\n",
    "    num_captions = 10\n",
    "    prompt = f'''Create a creative beautiful caption from this context:\n",
    "    \"This image is a {img_type}. There {ppl_result}.\n",
    "    The context is: {', '.join(terms_to_include)}.\n",
    "    The verb is: {v}\n",
    "    A creative short caption I can generate to describe this image is:'''\n",
    "\n",
    "    model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "    caption_texts = [flan_manager.generate_response(prompt, model_params) for _ in range(num_captions)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
