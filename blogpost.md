# Introduction

## Summary of Contributions

The paper "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language" [1] proposes a modular framework, called Socratic Models (SM), which employs multiple pretrained deep learning (DL) models to solve specific tasks. Such models range from pure language models (LM), whose input and output are exclusively textual, to visual-language (VLM) and audio-language models (ALM), which transform visual or audio information into text. In addition to these DL models, the framework can also incorporate modules that rely on the user's input or on specific APIs (e.g. robot actions). The framework uses prompting as a form of exchanging information between these pretrained models, the main benefit of such an approach being that it offers zero-shot multimodal reasoning without requiring task-specific fine-tuning.


As such, the main contribution of the paper is the Socratic Models framework. Additionaly, the paper covers various multimodal prompting methods and offers examples of tasks for which the multimodal capabilities of the SM are required. Therefore, the paper analyzes the zero-shot performance of specific SM configurations on tasks such as image captioning, contextual image description or video-to-text retrieval. All three tasks use the CLIP VLM [2] to extract information from the images, which is then passed to the GPT-3 LM [3] via prompting, whose role is to create a fitting caption or description. The video-to-text retrieval task also uses an ALM [4] to capture audio information which is also given to the LM. A quantitative analysis shows that SM have a higher performance on the zero-shot image captioning task compared to the state-of-the-art (SoTA) ZeroCap [5] but highly under-perform compared to fine-tuned methods such as ClipCap [6]. A similar trend can be seen for video-to-text retrieval, where SM outperform the zero-shot SoTA algorithms but under-perform when being compared to fine-tuned methods such as CLIP2Video [7]. As for the contextual image description task, SM managed to outperform even the fine-tuned method introduced by [8].


Finally, the paper also demonstrates the usage of SM in the context of three applications: egocentric perception, multimodal assistive dialogue and robot perception and planning. Such tasks make use of the user's input as well as APIs for web search and robot control policies. No quantitative evaluation is provided for these applications.

## Related Work

# Strenghts and Weaknesses

Strenghts:

1. The code was provided for the image captioning, video-to-text retrieval and robot perception and planning tasks
2. All deep learning models used in the paper are open-source and publicly available
3. The proposed framework outperforms zero-shot SoTA by just using pre-trained methods
4. The proposed framework is applicable for a various number of tasks since it provides multimodal
capabilities


Weaknesses:
1. No code was provided for the other tasks or applications. However, it should be fairly easy to adapt them
2. The  GPT-3 LM model is not freely accessible so moderate to high funds are needed
3. No proper evaluation was provided for the three applications



## References 
[1] Zeng, A. et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv
preprint arXiv:2204.00598 (2022).

[2] Radford, A. et al. Learning Transferable Visual Models From Natural Language Supervision 2021.
arXiv: 2103.00020 [cs.CV].

[3] Brown, T. et al. Language models are few-shot learners. Advances in neural information processing
systems 33, 1877â€“1901 (2020).

[4] Bapna, A. et al. mSLAM: Massively multilingual joint pre-training for speech and text 2022. arXiv:
2202.01374 [cs.CL].

[5] Tewel, Y., Shalev, Y., Schwartz, I. & Wolf, L. ZeroCap: Zero-Shot Image-to-Text Generation for
Visual-Semantic Arithmetic 2022. arXiv: 2111.14447 [cs.CV].

[6] Mokady, R., Hertz, A. & Bermano, A. H. ClipCap: CLIP Prefix for Image Captioning 2021. arXiv:
2111.09734 [cs.CV].

[7] Fang, H., Xiong, P., Xu, L. & Chen, Y. CLIP2Video: Mastering Video-Text Retrieval via Image
CLIP 2021. arXiv: 2106.11097 [cs.CV].

[8] Kreiss, E., Fang, F., Goodman, N. D. & Potts, C. Concadia: Towards Image-Based Text Generation
with a Purpose 2022. arXiv: 2104.08376 [cs.CL].









